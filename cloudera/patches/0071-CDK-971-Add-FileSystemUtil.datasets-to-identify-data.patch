From 0d92c7f989aa45b0997ada6a26ca596ef54ae3f7 Mon Sep 17 00:00:00 2001
From: Ryan Blue <blue@apache.org>
Date: Wed, 25 Mar 2015 21:35:12 -0700
Subject: [PATCH 071/115] CDK-971: Add FileSystemUtil.datasets to identify datasets.

This utility method identifies folders of data that appear to form a
dataset and have the same format and schema. Unknown files will prevent
a directory from being considered a dataset.
---
 .../data/spi/filesystem/FileSystemUtil.java        |  209 ++++++-
 .../data/spi/filesystem/TestFileSystemUtil.java    |  719 ++++++++++++++++++++
 2 files changed, 914 insertions(+), 14 deletions(-)
 create mode 100644 kite-data/kite-data-core/src/test/java/org/kitesdk/data/spi/filesystem/TestFileSystemUtil.java

diff --git a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemUtil.java b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemUtil.java
index 4cae78c..173e597 100644
--- a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemUtil.java
+++ b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemUtil.java
@@ -18,8 +18,10 @@ package org.kitesdk.data.spi.filesystem;
 
 import com.google.common.base.Preconditions;
 import com.google.common.base.Splitter;
+import com.google.common.collect.Iterables;
 import com.google.common.collect.Lists;
 import java.io.IOException;
+import java.util.Collection;
 import java.util.List;
 import javax.annotation.Nullable;
 import org.apache.avro.Schema;
@@ -32,6 +34,7 @@ import org.kitesdk.data.DatasetDescriptor;
 import org.kitesdk.data.DatasetIOException;
 import org.kitesdk.data.Format;
 import org.kitesdk.data.Formats;
+import org.kitesdk.data.IncompatibleSchemaException;
 import org.kitesdk.data.PartitionStrategy;
 import org.kitesdk.data.ValidationException;
 import org.kitesdk.data.spi.Pair;
@@ -253,7 +256,7 @@ public class FileSystemUtil {
     List<Pair<String, Class<? extends Comparable>>> pairs = visit(
         new GetPartitionInfo(), fs, location);
 
-    if (pairs.isEmpty() || pairs.size() <= 1) {
+    if (pairs == null || pairs.isEmpty() || pairs.size() <= 1) {
       return null;
     }
 
@@ -428,13 +431,7 @@ public class FileSystemUtil {
 
     @Override
     Format file(FileSystem fs, Path path) throws IOException {
-      String filename = path.getName();
-      for (Format format : SUPPORTED_FORMATS) {
-        if (filename.endsWith(format.getExtension())) {
-          return format;
-        }
-      }
-      return null;
+      return formatFromExt(path);
     }
   }
 
@@ -466,16 +463,200 @@ public class FileSystemUtil {
       }
       return null;
     }
+  }
+
+  /**
+   * Finds potential datasets by crawling a directory tree.
+   * <p>
+   * This method looks for any data files and directories appear to form a
+   * dataset. This deliberately ignores information that may be stored in the
+   * Hive metastore or .metadata folders.
+   * <p>
+   * Recognizes only Avro, Parquet, and JSON data files.
+   *
+   * @param fs a FileSystem for the root path
+   * @param path a root Path that will be searched
+   * @return a Collection with a DatasetDescriptor for each potential dataset.
+   * @throws IOException
+   */
+  public static Collection<DatasetDescriptor> findPotentialDatasets(
+      FileSystem fs, Path path) throws IOException {
+    List<DatasetDescriptor> descriptors = Lists.newArrayList();
+
+    Result result = visit(new FindDatasets(), fs, path);
+    if (result instanceof Result.Table) {
+      descriptors.add(descriptor(fs, (Result.Table) result));
+    } else if (result instanceof Result.Group) {
+      for (Result.Table table : ((Result.Group) result).tables) {
+        descriptors.add(descriptor(fs, table));
+      }
+    }
+
+    return descriptors;
+  }
+
+  private static DatasetDescriptor descriptor(FileSystem fs, Result.Table table)
+      throws IOException {
+    // inspect the path to determine the partition strategy
+    PartitionStrategy strategy = strategy(fs, table.location);
+    DatasetDescriptor.Builder builder = new DatasetDescriptor.Builder()
+        .format(table.format)
+        .schema(table.schema)
+        .partitionStrategy(strategy)
+        .location(table.location);
+    if (table.depth < 0) {
+      builder.property("kite.filesystem.mixed-depth", "true");
+    }
+    return builder.build();
+  }
+
+  private interface Result {
+    // An unknown data path
+    class Unknown implements Result {
+    }
+
+    // A table of data
+    class Table implements Result {
+      private static final int UNKNOWN_DEPTH = 0;
+      private static final int MIXED_DEPTH = -1;
+      private final Path location;
+      private final Format format;
+      private final Schema schema;
+      private final int depth;
+
+      public Table(Path location, Format format, Schema schema, int depth) {
+        this.location = location;
+        this.format = format;
+        this.schema = schema;
+        this.depth = depth;
+      }
+    }
+
+    // A group of tables
+    class Group implements Result {
+      private final List<Table> tables;
+      private final boolean containsUnknown;
+
+      public Group(List<Table> tables, boolean containsUnknown) {
+        this.tables = tables;
+        this.containsUnknown = containsUnknown;
+      }
+    }
+  }
+
+  private static class FindDatasets extends PathVisitor<Result> {
+    @Override
+    Result directory(FileSystem fs, Path path, List<Result> children) throws IOException {
+      // there are two possible outcomes for this method:
+      // 1. all child tables are compatible and part of one dataset (Table)
+      // 2. each valid child is a separate dataset (Group)
+      boolean allCompatible = true; // assume compatible to start
+      boolean containsUnknown = false;
+
+      // if all are compatible
+      Schema mergedSchema = null;
+      Format onlyFormat = null;
+      int depth = Result.Table.UNKNOWN_DEPTH;
+
+      // if all are separate datasets
+      List<Result.Table> tables = Lists.newArrayList();
+
+      for (Result child : children) {
+        if (child instanceof Result.Unknown) {
+          // not compatible at this level because a data file is not supported
+          allCompatible = false;
+          containsUnknown = true;
+
+        } else if (child instanceof Result.Group) {
+          // not compatible at a lower level
+          Result.Group group = (Result.Group) child;
+          containsUnknown |= group.containsUnknown;
+          if (containsUnknown || !group.tables.isEmpty()) {
+            // not compatible if there was an unknown or was not empty
+            allCompatible = false;
+          }
+          tables.addAll(group.tables);
+
+        } else {
+          Result.Table table = (Result.Table) child;
+          tables.add(table); // always add table in case not compatible later
+
+          // if all tables are currently compatible, add the latest table
+          if (allCompatible) {
+            try {
+              mergedSchema = merge(mergedSchema, table.schema);
+            } catch (IncompatibleSchemaException e) {
+              allCompatible = false;
+            }
+
+            if (onlyFormat == null) {
+              onlyFormat = table.format;
+            } else if (onlyFormat != table.format) {
+              allCompatible = false;
+            }
+
+            if (depth == Result.Table.UNKNOWN_DEPTH) {
+              depth = table.depth;
+            } else if (depth != table.depth) {
+              depth = Result.Table.MIXED_DEPTH;
+            }
+          }
+        }
+      }
 
-    private static Schema merge(@Nullable Schema left, @Nullable Schema right) {
-      if (left == null) {
-        return right;
-      } else if (right == null) {
-        return left;
+      if (allCompatible && tables.size() > 0) {
+        if (tables.size() == 1) {
+          // only one, use the existing location rather than higher up the path
+          return tables.get(0);
+        } else {
+          // more than one, use the path at this level
+          return new Result.Table(path, onlyFormat, mergedSchema, depth);
+        }
       } else {
-        return SchemaUtil.merge(left, right);
+        return new Result.Group(tables, containsUnknown);
       }
     }
+
+    @Override
+    Result file(FileSystem fs, Path path) throws IOException {
+      Format format = formatFromExt(path);
+      Schema schema = null;
+      if (format == Formats.AVRO) {
+        schema = Schemas.fromAvro(fs, path);
+      } else if (format == Formats.PARQUET) {
+        schema = Schemas.fromParquet(fs, path);
+      } else if (format == Formats.JSON) {
+        schema = Schemas.fromJSON("record", fs, path);
+      }
+
+      if (schema == null) {
+        return new Result.Unknown();
+      }
+
+      return new Result.Table(path, format, schema, path.depth());
+    }
   }
 
+  private static final Splitter DOT = Splitter.on('.');
+
+  private static Format formatFromExt(Path path) {
+    String filename = path.getName();
+    String ext = Iterables.getLast(DOT.split(filename));
+    for (Format format : SUPPORTED_FORMATS) {
+      if (ext.equals(format.getExtension())) {
+        return format;
+      }
+    }
+    return null;
+  }
+
+  private static Schema merge(@Nullable Schema left, @Nullable Schema right) {
+    if (left == null) {
+      return right;
+    } else if (right == null) {
+      return left;
+    } else {
+      return SchemaUtil.merge(left, right);
+    }
+  }
 }
diff --git a/kite-data/kite-data-core/src/test/java/org/kitesdk/data/spi/filesystem/TestFileSystemUtil.java b/kite-data/kite-data-core/src/test/java/org/kitesdk/data/spi/filesystem/TestFileSystemUtil.java
new file mode 100644
index 0000000..4901b3a
--- /dev/null
+++ b/kite-data/kite-data-core/src/test/java/org/kitesdk/data/spi/filesystem/TestFileSystemUtil.java
@@ -0,0 +1,719 @@
+/*
+ * Copyright 2013 Cloudera Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kitesdk.data.spi.filesystem;
+
+import com.google.common.collect.Iterables;
+import com.google.common.collect.Lists;
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.util.Collection;
+import java.util.UUID;
+import java.util.concurrent.Callable;
+import org.apache.avro.Schema;
+import org.apache.avro.SchemaBuilder;
+import org.apache.avro.generic.GenericData.Record;
+import org.apache.avro.generic.GenericRecord;
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.junit.Assert;
+import org.junit.BeforeClass;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.kitesdk.data.Dataset;
+import org.kitesdk.data.DatasetDescriptor;
+import org.kitesdk.data.DatasetWriter;
+import org.kitesdk.data.Datasets;
+import org.kitesdk.data.Formats;
+import org.kitesdk.data.LocalFileSystem;
+import org.kitesdk.data.PartitionStrategy;
+import org.kitesdk.data.TestHelpers;
+import org.kitesdk.data.spi.DescriptorUtil;
+
+import static org.kitesdk.data.CompressionType.Uncompressed;
+
+public class TestFileSystemUtil {
+  private static final Schema USER_SCHEMA = SchemaBuilder.record("User").fields()
+      .requiredLong("id")
+      .requiredString("username")
+      .endRecord();
+
+  private static final Schema EVENT_SCHEMA = SchemaBuilder.record("Event").fields()
+      .requiredLong("timestamp")
+      .requiredString("level")
+      .requiredString("message")
+      .endRecord();
+
+  private static final Record USER = new Record(USER_SCHEMA);
+  private static final Record EVENT = new Record(EVENT_SCHEMA);
+
+  @BeforeClass
+  public static void initRecords() {
+    USER.put("id", 1L);
+    USER.put("username", "test");
+    EVENT.put("timestamp", System.currentTimeMillis());
+    EVENT.put("level", "DEBUG");
+    EVENT.put("message", "Useless information!");
+  }
+
+  @Rule
+  public TemporaryFolder temp = new TemporaryFolder();
+
+  @Test
+  public void testEmptyDataset() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e/dataset_name");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+    URI datasetUri = URI.create("dataset:file:" + folder.getAbsolutePath());
+    DatasetDescriptor descriptor = new DatasetDescriptor.Builder()
+        .schema(USER_SCHEMA)
+        .build();
+
+    Datasets.create(datasetUri, descriptor);
+
+    Collection<DatasetDescriptor> expected = Lists.newArrayList();
+    Assert.assertEquals("Should succeed and find no datasets",
+        expected, FileSystemUtil.findPotentialDatasets(fs, root));
+  }
+
+  @Test
+  public void testUnpartitionedDataset() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e/dataset_name");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+    URI datasetUri = URI.create("dataset:file:" + folder.getAbsolutePath());
+    DatasetDescriptor descriptor = new DatasetDescriptor.Builder()
+        .schema(USER_SCHEMA)
+        .build();
+
+    Dataset<GenericRecord> dataset = Datasets.create(datasetUri, descriptor);
+
+    // write two so that the descriptor uses the directory rather than a file
+    writeUserToDataset(dataset);
+    writeUserToDataset(dataset);
+
+    DatasetDescriptor expected = dataset.getDescriptor();
+    DatasetDescriptor actual = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertEquals("Should succeed and find an equivalent descriptor",
+        expected, actual);
+  }
+
+  @Test
+  public void testPartitionedDataset() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e/dataset_name");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+    URI datasetUri = URI.create("dataset:file:" + folder.getAbsolutePath());
+    DatasetDescriptor descriptor = new DatasetDescriptor.Builder()
+        .schema(USER_SCHEMA)
+        .partitionStrategy(new PartitionStrategy.Builder()
+            .hash("id", 4)
+            .build())
+        .build();
+
+    Dataset<GenericRecord> dataset = Datasets.create(datasetUri, descriptor);
+
+    // write two so that the descriptor uses the directory rather than a file
+    writeUserToDataset(dataset);
+    writeUserToDataset(dataset);
+
+    Path datasetPath = new Path(folder.toURI());
+    Path partitionPath = new Path(datasetPath, "id_hash=1");
+
+    DatasetDescriptor actual = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Location should be at the partition directory",
+        partitionPath.toUri(), actual.getLocation());
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, actual.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, actual.getFormat());
+    Assert.assertFalse("Should not be partitioned", actual.isPartitioned());
+  }
+
+  @Test
+  public void testSingleAvroFile() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a single Avro file
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri().getPath(),
+        parent(descriptor.getLocation()).getPath());
+    Assert.assertTrue("Should be a .avro file",
+        descriptor.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, descriptor.getFormat());
+    Assert.assertFalse("Should not be partitioned", descriptor.isPartitioned());
+  }
+
+  @Test
+  public void testMultipleAvroFilesInOneFolder() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files in parent
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, parent);
+    createAvroUserFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, descriptor.getFormat());
+    Assert.assertFalse("Should not be partitioned", descriptor.isPartitioned());
+  }
+
+  @Test
+  public void testMultipleAvroFilesInSeparateFolders() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroUserFile(fs, new Path(parent, "2"));
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    PartitionStrategy strategy = new PartitionStrategy.Builder()
+        .provided("part", "int")
+        .build();
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, descriptor.getFormat());
+    Assert.assertEquals("Should be partitioned by part=int",
+        strategy, descriptor.getPartitionStrategy());
+  }
+
+  @Test
+  public void testMultipleAvroFilesAtDifferentDepths() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroUserFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    PartitionStrategy strategy = new PartitionStrategy.Builder()
+        .provided("part", "int")
+        .build();
+
+    Assert.assertTrue("Should flag data at mixed depth in the directory tree",
+        DescriptorUtil.isEnabled("kite.filesystem.mixed-depth", descriptor));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, descriptor.getFormat());
+    Assert.assertEquals("Should be partitioned by part=int",
+        strategy, descriptor.getPartitionStrategy());
+  }
+
+  @Test
+  public void testMultipleMergeTablesAtDifferentDepths() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroUserFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    PartitionStrategy strategy = new PartitionStrategy.Builder()
+        .provided("part", "int")
+        .build();
+
+    Assert.assertTrue("Should flag data at mixed depth in the directory tree",
+        DescriptorUtil.isEnabled("kite.filesystem.mixed-depth", descriptor));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, descriptor.getFormat());
+    Assert.assertEquals("Should be partitioned by part=int",
+        strategy, descriptor.getPartitionStrategy());
+  }
+
+  @Test
+  public void testMultipleAvroFilesInSeparateFoldersWithUnknown() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders, with an unknown
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroUserFile(fs, new Path(parent, "part=2"));
+    createUnknownFile(fs, new Path(parent, "part=3"));
+
+    Collection<DatasetDescriptor> descriptors = FileSystemUtil
+        .findPotentialDatasets(fs, root);
+
+    Assert.assertEquals("Should have 2 descriptors", 2, descriptors.size());
+    DatasetDescriptor users1;
+    DatasetDescriptor users2;
+    DatasetDescriptor first = Iterables.getFirst(descriptors, null);
+    if (first.getLocation().toString().contains("part=1")) {
+      users1 = first;
+      users2 = Iterables.getLast(descriptors, null);
+    } else {
+      users2 = first;
+      users1 = Iterables.getLast(descriptors, null);
+    }
+
+    // the descriptors may be out of order, so check and swap
+    if (users1.getLocation().toString().contains("part=2")) {
+      users2 = Iterables.getFirst(descriptors, null);
+      users1 = Iterables.getLast(descriptors, null);
+    }
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        users1.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=1").toUri(),
+        parent(users1.getLocation()));
+    Assert.assertTrue("Should be a .avro file",
+        users1.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, users1.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, users1.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        users1.isPartitioned());
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        users2.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=2").toUri(),
+        parent(users2.getLocation()));
+    Assert.assertTrue("Should be a .avro file",
+        users2.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, users2.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, users2.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        users2.isPartitioned());
+  }
+
+  @Test
+  public void testSingleParquetFile() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a single Avro file
+    Path parent = new Path(folder.toURI());
+    createParquetEventFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri().getPath(),
+        parent(descriptor.getLocation()).getPath());
+    Assert.assertTrue("Should be a .parquet file",
+        descriptor.getLocation().toString().endsWith(".parquet"));
+    Assert.assertEquals("Should use event schema",
+        EVENT_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Parquet format",
+        Formats.PARQUET, descriptor.getFormat());
+    Assert.assertFalse("Should not be partitioned", descriptor.isPartitioned());
+  }
+
+  @Test
+  public void testMultipleParquetFilesInOneFolder() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a single Avro file
+    Path parent = new Path(folder.toURI());
+    createParquetEventFile(fs, parent);
+    createParquetEventFile(fs, parent);
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use event schema",
+        EVENT_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Parquet format",
+        Formats.PARQUET, descriptor.getFormat());
+    Assert.assertFalse("Should not be partitioned", descriptor.isPartitioned());
+  }
+
+  @Test
+  public void testMultipleParquetFilesInSeparateFolders() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders
+    Path parent = new Path(folder.toURI());
+    createParquetEventFile(fs, new Path(parent, "part"));
+    createParquetEventFile(fs, new Path(parent, "2"));
+
+    DatasetDescriptor descriptor = Iterables.getOnlyElement(
+        FileSystemUtil.findPotentialDatasets(fs, root));
+
+    PartitionStrategy strategy = new PartitionStrategy.Builder()
+        .provided("partition_1", "string")
+        .build();
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        descriptor.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(), descriptor.getLocation());
+    Assert.assertEquals("Should use user schema",
+        EVENT_SCHEMA, descriptor.getSchema());
+    Assert.assertEquals("Should have Parquet format",
+        Formats.PARQUET, descriptor.getFormat());
+    Assert.assertEquals("Should be partitioned by part=int",
+        strategy, descriptor.getPartitionStrategy());
+  }
+
+  @Test
+  public void testIncompatibleSchemaFilesInSeparateFolders() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Avro files under separate folders, with different schemas
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, new Path(parent, "part=1"));
+    createAvroEventFile(fs, new Path(parent, "part=2"));
+
+    Collection<DatasetDescriptor> descriptors = FileSystemUtil
+        .findPotentialDatasets(fs, root);
+
+    Assert.assertEquals("Should have 2 descriptors", 2, descriptors.size());
+    DatasetDescriptor users;
+    DatasetDescriptor events;
+    DatasetDescriptor first = Iterables.getFirst(descriptors, null);
+    if (first.getLocation().toString().contains("part=1")) {
+      users = first;
+      events = Iterables.getLast(descriptors, null);
+    } else {
+      events = first;
+      users = Iterables.getLast(descriptors, null);
+    }
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        users.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=1").toUri(),
+        parent(users.getLocation()));
+    Assert.assertTrue("Should be a .avro file",
+        users.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, users.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, users.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        users.isPartitioned());
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        events.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=2").toUri(),
+        parent(events.getLocation()));
+    Assert.assertTrue("Should be a .avro file",
+        events.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use event schema",
+        EVENT_SCHEMA, events.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, events.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        events.isPartitioned());
+  }
+
+  @Test
+  public void testIncompatibleSchemaParquetFilesInSeparateFolders() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a two Parquet files under separate folders, with different schemas
+    Path parent = new Path(folder.toURI());
+    createParquetUserFile(fs, new Path(parent, "part=1"));
+    createParquetEventFile(fs, new Path(parent, "part=2"));
+
+    Collection<DatasetDescriptor> descriptors = FileSystemUtil
+        .findPotentialDatasets(fs, root);
+
+    Assert.assertEquals("Should have 2 descriptors", 2, descriptors.size());
+    DatasetDescriptor users;
+    DatasetDescriptor events;
+    DatasetDescriptor first = Iterables.getFirst(descriptors, null);
+    if (first.getLocation().toString().contains("part=1")) {
+      users = first;
+      events = Iterables.getLast(descriptors, null);
+    } else {
+      events = first;
+      users = Iterables.getLast(descriptors, null);
+    }
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        users.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=1").toUri(),
+        parent(users.getLocation()));
+    Assert.assertTrue("Should be a .parquet file",
+        users.getLocation().toString().endsWith(".parquet"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, users.getSchema());
+    Assert.assertEquals("Should have Parquet format",
+        Formats.PARQUET, users.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        users.isPartitioned());
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        events.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        new Path(parent, "part=2").toUri(),
+        parent(events.getLocation()));
+    Assert.assertTrue("Should be a .parquet file",
+        events.getLocation().toString().endsWith(".parquet"));
+    Assert.assertEquals("Should use event schema",
+        EVENT_SCHEMA, events.getSchema());
+    Assert.assertEquals("Should have Parquet format",
+        Formats.PARQUET, events.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        events.isPartitioned());
+  }
+
+  @Test
+  public void testIncompatibleFormatFilesInSameFolder() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create Avro and Parquet files under separate folders, with the same schema
+    Path parent = new Path(folder.toURI());
+    createAvroUserFile(fs, parent);
+    createParquetUserFile(fs, parent);
+
+    Collection<DatasetDescriptor> descriptors = FileSystemUtil
+        .findPotentialDatasets(fs, root);
+
+    Assert.assertEquals("Should have 2 descriptors", 2, descriptors.size());
+    DatasetDescriptor avro;
+    DatasetDescriptor parquet;
+    DatasetDescriptor first = Iterables.getFirst(descriptors, null);
+    if (first.getFormat() == Formats.AVRO) {
+      avro = first;
+      parquet = Iterables.getLast(descriptors, null);
+    } else {
+      parquet = first;
+      avro = Iterables.getLast(descriptors, null);
+    }
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        avro.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(),
+        parent(avro.getLocation()));
+    Assert.assertTrue("Should be a .avro file",
+        avro.getLocation().toString().endsWith(".avro"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, avro.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.AVRO, avro.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        avro.isPartitioned());
+
+    Assert.assertFalse("Should not flag at mixed depth",
+        parquet.hasProperty("kite.filesystem.mixed-depth"));
+    Assert.assertEquals("Should be directly under parent",
+        parent.toUri(),
+        parent(parquet.getLocation()));
+    Assert.assertTrue("Should be a .parquet file",
+        parquet.getLocation().toString().endsWith(".parquet"));
+    Assert.assertEquals("Should use user schema",
+        USER_SCHEMA, parquet.getSchema());
+    Assert.assertEquals("Should have Avro format",
+        Formats.PARQUET, parquet.getFormat());
+    Assert.assertFalse("Should not be partitioned",
+        parquet.isPartitioned());
+  }
+
+  @Test
+  public void testSingleUnknownFile() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a single Avro file
+    Path parent = new Path(folder.toURI());
+    createUnknownFile(fs, parent);
+
+    Collection<DatasetDescriptor> expected = Lists.newArrayList();
+    Assert.assertEquals("Should succeed and find no datasets",
+        expected, FileSystemUtil.findPotentialDatasets(fs, root));
+  }
+
+  @Test
+  public void testMultipleUnknownFiles() throws Exception {
+    File folder = temp.newFolder("a/b/c/d/e");
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+
+    // create a single Avro file
+    Path parent = new Path(folder.toURI());
+    createUnknownFile(fs, parent);
+    createUnknownFile(fs, parent);
+
+    Collection<DatasetDescriptor> expected = Lists.newArrayList();
+    Assert.assertEquals("Should succeed and find no datasets",
+        expected, FileSystemUtil.findPotentialDatasets(fs, root));
+  }
+
+  @Test
+  public void testEmptyDirectory() throws IOException {
+    Path root = new Path(temp.getRoot().toURI());
+    FileSystem fs = LocalFileSystem.getInstance();
+    Collection<DatasetDescriptor> expected = Lists.newArrayList();
+    Assert.assertEquals("Should succeed and find no datasets",
+        expected, FileSystemUtil.findPotentialDatasets(fs, root));
+  }
+
+  @Test
+  public void testMissingDirectory() throws IOException {
+    final Path root = new Path(new Path(temp.getRoot().toURI()), "not_there");
+    final FileSystem fs = LocalFileSystem.getInstance();
+
+    TestHelpers.assertThrows("Should propagate missing file IOException",
+        FileNotFoundException.class, new Callable<Void>() {
+          @Override
+          public Void call() throws IOException {
+            FileSystemUtil.findPotentialDatasets(fs, root);
+            return null;
+          }
+        });
+  }
+
+  private URI parent(URI file) {
+    return new Path(file).getParent().toUri();
+  }
+
+  public void writeUserToDataset(Dataset<GenericRecord> dataset) {
+    DatasetWriter<GenericRecord> writer = null;
+    try {
+      writer = dataset.newWriter();
+      writer.write(USER);
+    } finally {
+      if (writer != null) {
+        writer.close();
+      }
+    }
+  }
+
+  public void createAvroUserFile(FileSystem fs, Path parent) throws IOException {
+    Path file = new Path(parent, UUID.randomUUID().toString() + ".avro");
+    AvroAppender<Record> appender = new AvroAppender<Record>(
+        fs, file, USER_SCHEMA, Uncompressed);
+    appender.open();
+    appender.append(USER);
+    appender.close();
+  }
+
+  public void createAvroEventFile(FileSystem fs, Path parent) throws IOException {
+    Path file = new Path(parent, UUID.randomUUID().toString() + ".avro");
+    AvroAppender<Record> appender = new AvroAppender<Record>(
+        fs, file, EVENT_SCHEMA, Uncompressed);
+    appender.open();
+    appender.append(EVENT);
+    appender.close();
+  }
+
+  public void createParquetUserFile(FileSystem fs, Path parent) throws IOException {
+    Path file = new Path(parent, UUID.randomUUID().toString() + ".parquet");
+    ParquetAppender<Record> appender = new ParquetAppender<Record>(
+        fs, file, USER_SCHEMA, new Configuration(), Uncompressed);
+    appender.open();
+    appender.append(USER);
+    appender.close();
+  }
+
+  public void createParquetEventFile(FileSystem fs, Path parent) throws IOException {
+    Path file = new Path(parent, UUID.randomUUID().toString() + ".parquet");
+    ParquetAppender<Record> appender = new ParquetAppender<Record>(
+        fs, file, EVENT_SCHEMA, new Configuration(), Uncompressed);
+    appender.open();
+    appender.append(EVENT);
+    appender.close();
+  }
+
+  public void createUnknownFile(FileSystem fs, Path parent) throws IOException {
+    Path file = new Path(parent, UUID.randomUUID().toString() + ".unknown");
+    fs.create(file);
+  }
+}
-- 
1.7.0.4

