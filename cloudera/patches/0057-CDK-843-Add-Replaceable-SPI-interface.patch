From 6baf75cbb6415af47fa33748289258a078761a70 Mon Sep 17 00:00:00 2001
From: Ryan Blue <blue@apache.org>
Date: Mon, 20 Apr 2015 18:12:06 -0700
Subject: [PATCH 057/140] CDK-843: Add Replaceable SPI interface.

This is like the Mergeable interface, but tests whether the dataset
supports replacement. It also includes a method to test whether a view
can be replaced.
---
 .../java/org/kitesdk/data/spi/Replaceable.java     |   41 ++++++++++++++++++++
 .../data/spi/filesystem/FileSystemDataset.java     |   37 ++++++++++++------
 .../spi/filesystem/FileSystemPartitionView.java    |    2 +-
 .../org/kitesdk/data/crunch/DatasetTarget.java     |   22 +++++++++--
 .../data/mapreduce/DatasetKeyOutputFormat.java     |   31 ++++++++++++---
 .../org/kitesdk/data/mapreduce/TestMapReduce.java  |    2 +-
 .../java/org/kitesdk/cli/commands/CopyCommand.java |   10 +++++
 .../main/java/org/kitesdk/tools/TransformTask.java |   11 +++++-
 8 files changed, 132 insertions(+), 24 deletions(-)
 create mode 100644 kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/Replaceable.java

diff --git a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/Replaceable.java b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/Replaceable.java
new file mode 100644
index 0000000..adf5462
--- /dev/null
+++ b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/Replaceable.java
@@ -0,0 +1,41 @@
+/*
+ * Copyright 2015 Cloudera Inc.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.kitesdk.data.spi;
+
+/**
+ * This interface is for classes that can replace parts of themselves in some
+ * (undefined) way. Once replaced, the update can be discarded without losing
+ * information.
+ *
+ * @param <T> the type of the object to replace
+ */
+public interface Replaceable<T> {
+  /**
+   * Check whether {@code part} can be replaced.
+   *
+   * @param part the object to replace parts of this
+   * @return {@code true} if the object can replace parts of this
+   */
+  public boolean canReplace(T part);
+
+  /**
+   * Replace part of {@code this} with the {@code replacement} object.
+   *
+   * @param replacement the object to replace parts of this
+   */
+  public void replace(T replacement);
+}
diff --git a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemDataset.java b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemDataset.java
index b4679cf..8e99aa3 100644
--- a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemDataset.java
+++ b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemDataset.java
@@ -43,6 +43,7 @@ import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.kitesdk.data.spi.PartitionedDataset;
+import org.kitesdk.data.spi.Replaceable;
 import org.kitesdk.data.spi.SizeAccessor;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -57,7 +58,7 @@ import org.kitesdk.data.Formats;
 @SuppressWarnings("deprecation")
 public class FileSystemDataset<E> extends AbstractDataset<E> implements
     Mergeable<FileSystemDataset<E>>, InputFormatAccessor<E>, LastModifiedAccessor,
-    PartitionedDataset<E>, SizeAccessor, Signalable<E> {
+    PartitionedDataset<E>, SizeAccessor, Signalable<E>, Replaceable<View<E>> {
 
   private static final Logger LOG = LoggerFactory
     .getLogger(FileSystemDataset.class);
@@ -218,8 +219,8 @@ public class FileSystemDataset<E> extends AbstractDataset<E> implements
       "Attempt to get a partition on a non-partitioned dataset (name:%s)",
       name);
 
-    LOG.debug("Loading partition for key {}, allowCreate:{}", new Object[] {
-      key, allowCreate });
+    LOG.debug("Loading partition for key {}, allowCreate:{}", new Object[]{
+        key, allowCreate});
 
     Path partitionDirectory = fileSystem.makeQualified(
         toDirectoryName(directory, key));
@@ -327,7 +328,7 @@ public class FileSystemDataset<E> extends AbstractDataset<E> implements
   }
 
   public void addExistingPartitions() {
-    if (partitionListener != null) {
+    if (partitionListener != null && descriptor.isPartitioned()) {
       for (Path partition : pathIterator()) {
         partitionListener.partitionAdded(namespace, name, partition.toString());
       }
@@ -377,7 +378,19 @@ public class FileSystemDataset<E> extends AbstractDataset<E> implements
     }
   }
 
-  public void replace(FileSystemView<E> update) {
+  @Override
+  public boolean canReplace(View<E> part) {
+    if (part instanceof FileSystemView) {
+      return equals(part.getDataset()) &&
+          ((FileSystemView) part).getConstraints().alignedWithBoundaries();
+    } else if (part instanceof FileSystemDataset) {
+      return equals(part);
+    }
+    return false;
+  }
+
+  @Override
+  public void replace(View<E> update) {
     DatasetDescriptor updateDescriptor = update.getDataset().getDescriptor();
 
     // check that the dataset's descriptor can read the update
@@ -397,12 +410,14 @@ public class FileSystemDataset<E> extends AbstractDataset<E> implements
         Iterable<PartitionView<E>> existingPartitions = dest
             .toConstraintsView()
             .getCoveringPartitions();
-        for (PartitionView<E> toRemove : existingPartitions) {
-          Path path = new Path(toRemove.getUri());
+        for (PartitionView<E> partition : existingPartitions) {
+          FileSystemPartitionView<E> toRemove =
+              (FileSystemPartitionView<E>) partition;
+          Path path = new Path(toRemove.getLocation());
           removals.add(path);
-          if (partitionListener != null) {
+          if (partitionListener != null && descriptor.isPartitioned()) {
             partitionListener.partitionDeleted(
-                namespace, name, path.toString());
+                namespace, name, toRemove.getRelativeLocation().toString());
           }
         }
 
@@ -411,9 +426,9 @@ public class FileSystemDataset<E> extends AbstractDataset<E> implements
             new Path(dest.getLocation()), new Path(src.getLocation()),
             removals);
 
-        if (partitionListener != null) {
+        if (partitionListener != null && descriptor.isPartitioned()) {
           partitionListener.partitionAdded(
-              namespace, name, dest.getLocation().toString());
+              namespace, name, dest.getRelativeLocation().toString());
         }
 
       } else {
diff --git a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemPartitionView.java b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemPartitionView.java
index bf9f9a4..58bb2fb 100644
--- a/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemPartitionView.java
+++ b/kite-data/kite-data-core/src/main/java/org/kitesdk/data/spi/filesystem/FileSystemPartitionView.java
@@ -156,7 +156,7 @@ class FileSystemPartitionView<E> extends FileSystemView<E>
   @edu.umd.cs.findbugs.annotations.SuppressWarnings(
       value="NP_PARAMETER_MUST_BE_NONNULL_BUT_MARKED_AS_NULLABLE",
       justification="Null value checked by precondition")
-  private static URI relativize(@Nullable URI root, URI location) {
+  private static URI relativize(@Nullable URI root, @Nullable URI location) {
     Preconditions.checkNotNull(root, "Cannot find location relative to null");
 
     if (location == null) {
diff --git a/kite-data/kite-data-crunch/src/main/java/org/kitesdk/data/crunch/DatasetTarget.java b/kite-data/kite-data-crunch/src/main/java/org/kitesdk/data/crunch/DatasetTarget.java
index cac0688..359de8e 100644
--- a/kite-data/kite-data-crunch/src/main/java/org/kitesdk/data/crunch/DatasetTarget.java
+++ b/kite-data/kite-data-crunch/src/main/java/org/kitesdk/data/crunch/DatasetTarget.java
@@ -76,6 +76,9 @@ class DatasetTarget<E> implements MapReduceTarget {
   @SuppressWarnings("unchecked")
   public boolean handleExisting(WriteMode writeMode, long lastModForSource,
       Configuration entries) {
+    outputConf(
+        DatasetKeyOutputFormat.KITE_WRITE_MODE,
+        kiteWriteMode(writeMode).toString());
 
     if (view == null) {
       try {
@@ -98,11 +101,10 @@ class DatasetTarget<E> implements MapReduceTarget {
           LOG.error("Dataset/view " + view + " already exists!");
           throw new CrunchRuntimeException("Dataset/view already exists: " + view);
         case OVERWRITE:
-          LOG.info("Deleting all data from: " + view);
-          delete(view);
+          LOG.info("Overwriting existing dataset/view: " + view);
           break;
         case APPEND:
-          LOG.info("Writing to existing dataset/view: " + view);
+          LOG.info("Appending to existing dataset/view: " + view);
           break;
         case CHECKPOINT:
           long lastModForTarget = -1;
@@ -133,6 +135,20 @@ class DatasetTarget<E> implements MapReduceTarget {
     return exists;
   }
 
+  private DatasetKeyOutputFormat.WriteMode kiteWriteMode(WriteMode mode) {
+    switch (mode) {
+      case DEFAULT:
+        return DatasetKeyOutputFormat.WriteMode.DEFAULT;
+      case APPEND:
+        return DatasetKeyOutputFormat.WriteMode.APPEND;
+      case OVERWRITE:
+        return DatasetKeyOutputFormat.WriteMode.OVERWRITE;
+      default:
+        // use APPEND and enforce in handleExisting
+        return DatasetKeyOutputFormat.WriteMode.APPEND;
+    }
+  }
+
   private void delete(View view) {
     try {
       boolean deleted = view.deleteAll();
diff --git a/kite-data/kite-data-mapreduce/src/main/java/org/kitesdk/data/mapreduce/DatasetKeyOutputFormat.java b/kite-data/kite-data-mapreduce/src/main/java/org/kitesdk/data/mapreduce/DatasetKeyOutputFormat.java
index da57ca0..dc03ef7 100644
--- a/kite-data/kite-data-mapreduce/src/main/java/org/kitesdk/data/mapreduce/DatasetKeyOutputFormat.java
+++ b/kite-data/kite-data-mapreduce/src/main/java/org/kitesdk/data/mapreduce/DatasetKeyOutputFormat.java
@@ -50,6 +50,7 @@ import org.kitesdk.data.spi.DatasetRepository;
 import org.kitesdk.data.spi.Mergeable;
 import org.kitesdk.data.spi.PartitionKey;
 import org.kitesdk.data.spi.Registration;
+import org.kitesdk.data.spi.Replaceable;
 import org.kitesdk.data.spi.TemporaryDatasetRepository;
 import org.kitesdk.data.spi.TemporaryDatasetRepositoryAccessor;
 import org.kitesdk.data.spi.filesystem.FileSystemDataset;
@@ -68,14 +69,14 @@ public class DatasetKeyOutputFormat<E> extends OutputFormat<E, Void> {
   public static final String KITE_OUTPUT_URI = "kite.outputUri";
   public static final String KITE_PARTITION_DIR = "kite.outputPartitionDir";
   public static final String KITE_TYPE = "kite.outputEntityType";
+  public static final String KITE_WRITE_MODE = "kite.outputMode";
 
-  private static final String KITE_WRITE_MODE = "kite.outputMode";
-  private static final String TEMP_NAMESPACE = "mr";
-
-  private static enum WriteMode {
+  public static enum WriteMode {
     DEFAULT, APPEND, OVERWRITE
   }
 
+  private static final String TEMP_NAMESPACE = "mr";
+
   public static class ConfigBuilder {
     private final Configuration conf;
 
@@ -372,13 +373,20 @@ public class DatasetKeyOutputFormat<E> extends OutputFormat<E, Void> {
     @Override
     @SuppressWarnings("unchecked")
     public void commitJob(JobContext jobContext) throws IOException {
+      Configuration conf = Hadoop.JobContext
+          .getConfiguration.invoke(jobContext);
       DatasetRepository repo = getDatasetRepository(jobContext);
       boolean isTemp = repo instanceof TemporaryDatasetRepository;
 
       String jobDatasetName = getJobDatasetName(jobContext);
       View<E> targetView = load(jobContext);
       Dataset<E> jobDataset = repo.load(TEMP_NAMESPACE, jobDatasetName);
-      ((Mergeable<Dataset<E>>) targetView.getDataset()).merge(jobDataset);
+      WriteMode mode = conf.getEnum(KITE_WRITE_MODE, WriteMode.DEFAULT);
+      if (mode == WriteMode.OVERWRITE && canReplace(targetView)) {
+        ((Replaceable<Dataset<E>>) targetView.getDataset()).replace(jobDataset);
+      } else {
+        ((Mergeable<Dataset<E>>) targetView.getDataset()).merge(jobDataset);
+      }
 
       if (targetView instanceof Signalable) {
         ((Signalable)targetView).signalReady();
@@ -470,7 +478,8 @@ public class DatasetKeyOutputFormat<E> extends OutputFormat<E, Void> {
       case APPEND:
         break;
       case OVERWRITE:
-        if (!target.isEmpty()) {
+        // if the merge won't use replace, then delete the existing data
+        if (!canReplace(target)) {
           target.deleteAll();
         }
         break;
@@ -634,4 +643,14 @@ public class DatasetKeyOutputFormat<E> extends OutputFormat<E, Void> {
         .build();
   }
 
+  @SuppressWarnings("unchecked")
+  private static boolean canReplace(View<?> view) {
+    if (Hadoop.isHadoop1()) {
+      // can't use replace because it is called in the OutputCommitter.
+      return false;
+    }
+    Dataset<?> dataset = view.getDataset();
+    return (dataset instanceof Replaceable &&
+        ((Replaceable<View<?>>) dataset).canReplace(view));
+  }
 }
diff --git a/kite-data/kite-data-mapreduce/src/test/java/org/kitesdk/data/mapreduce/TestMapReduce.java b/kite-data/kite-data-mapreduce/src/test/java/org/kitesdk/data/mapreduce/TestMapReduce.java
index 5d85823..be3bbdb 100644
--- a/kite-data/kite-data-mapreduce/src/test/java/org/kitesdk/data/mapreduce/TestMapReduce.java
+++ b/kite-data/kite-data-mapreduce/src/test/java/org/kitesdk/data/mapreduce/TestMapReduce.java
@@ -214,7 +214,7 @@ public class TestMapReduce extends FileSystemTestBase {
     job.setReducerClass(GenericStatsReducer.class);
 
     View<Record> outputView = outputDataset.with("name", "apple", "banana", "carrot");
-    DatasetKeyOutputFormat.configure(job).overwrite(outputView).withType(GenericData.Record.class);
+    DatasetKeyOutputFormat.configure(job).appendTo(outputView).withType(GenericData.Record.class);
 
     Assert.assertTrue(job.waitForCompletion(true));
 
diff --git a/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/cli/commands/CopyCommand.java b/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/cli/commands/CopyCommand.java
index c559d20..479f3f8 100644
--- a/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/cli/commands/CopyCommand.java
+++ b/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/cli/commands/CopyCommand.java
@@ -23,6 +23,7 @@ import com.google.common.collect.Lists;
 import java.io.IOException;
 import java.util.List;
 import org.apache.crunch.PipelineResult;
+import org.apache.crunch.Target;
 import org.kitesdk.data.View;
 import org.kitesdk.tools.CopyTask;
 import org.slf4j.Logger;
@@ -47,6 +48,11 @@ public class CopyCommand extends BaseDatasetCommand {
       description="The number of writer processes to use")
   int numWriters = -1;
 
+  @Parameter(
+      names={"--overwrite"},
+      description="Remove any data already in the target view or dataset")
+  boolean overwrite = false;
+
   @Override
   public int run() throws IOException {
     Preconditions.checkArgument(datasets != null && datasets.size() > 1,
@@ -69,6 +75,10 @@ public class CopyCommand extends BaseDatasetCommand {
       task.setNumWriters(numWriters);
     }
 
+    if (overwrite) {
+      task.setWriteMode(Target.WriteMode.OVERWRITE);
+    }
+
     PipelineResult result = task.run();
 
     if (result.succeeded()) {
diff --git a/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/tools/TransformTask.java b/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/tools/TransformTask.java
index 8b0a5cf..ef66999 100644
--- a/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/tools/TransformTask.java
+++ b/kite-tools-parent/kite-tools/src/main/java/org/kitesdk/tools/TransformTask.java
@@ -34,7 +34,6 @@ import org.apache.crunch.types.avro.AvroType;
 import org.apache.crunch.types.avro.Avros;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.kitesdk.compat.DynMethods;
 import org.kitesdk.data.Dataset;
@@ -63,6 +62,7 @@ public class TransformTask<S, T> extends Configured {
   private final DoFn<S, T> transform;
   private boolean compact = true;
   private int numWriters = -1;
+  private Target.WriteMode mode = Target.WriteMode.APPEND;
 
   private long count = 0;
 
@@ -93,6 +93,13 @@ public class TransformTask<S, T> extends Configured {
     return this;
   }
 
+  public TransformTask setWriteMode(Target.WriteMode mode) {
+    Preconditions.checkArgument(mode != Target.WriteMode.CHECKPOINT,
+        "Checkpoint is not an allowed write mode");
+    this.mode = mode;
+    return this;
+  }
+
   public PipelineResult run() throws IOException {
     if (isLocal(from.getDataset()) || isLocal(to.getDataset())) {
       // copy to avoid making changes to the caller's configuration
@@ -121,7 +128,7 @@ public class TransformTask<S, T> extends Configured {
       collection = CrunchDatasets.partition(collection, to, numWriters);
     }
 
-    pipeline.write(collection, CrunchDatasets.asTarget(to), Target.WriteMode.APPEND);
+    pipeline.write(collection, CrunchDatasets.asTarget(to), mode);
 
     PipelineResult result = pipeline.done();
 
-- 
1.7.9.5

